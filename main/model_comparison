import os
import sys
import glob
import gymnasium as gym
import highway_env
import numpy as np
from collections import defaultdict
import torch
import hydra
import matplotlib.pyplot as plt
from pathlib import Path

# Add the project root directory to Python path
project_root = str(Path(__file__).parent.parent)
if project_root not in sys.path:
    sys.path.append(project_root)

from agents.pure_mpc import PureMPC_Agent
from agents.pure_mpc_no_collision import PureMPC_Agent as PureMPC_NoCollision_Agent
from config.config import build_env_config, build_mpcrl_agent_config, build_pure_mpc_agent_config
from trainers.trainer import RefSpeedTrainer

# Evaluation Configuration
VISUALIZATION_MODE = False  # Set to False for multiple runs without visualization
N_EPISODES = 1 if VISUALIZATION_MODE else 10  # Number of episodes to run
RENDER_MODE = "human" if VISUALIZATION_MODE else "rgb_array"
MODELS_TO_EVALUATE = ["pure_mpc_no_collision"]  # Options: ["pure_mpc", "pure_mpc_no_collision", "mpcrl"]
MODELS_TO_EVALUATE = ["pure_mpc", "pure_mpc_no_collision", "mpcrl"]

class ModelEvaluator:
    def __init__(self, cfg):
        self.cfg = cfg
        self.env_config = build_env_config(cfg)
        self.pure_mpc_cfg = build_pure_mpc_agent_config(cfg)
        self.pure_mpc_no_collision_cfg = build_pure_mpc_agent_config(cfg, use_collision_avoidance=False)
        self.mpcrl_cfg = build_mpcrl_agent_config(cfg, version="v0", algorithm="ppo")

    def evaluate_single_episode(self, model_type, model=None, env=None):
        """Run a single episode and return metrics"""
        observation, _ = env.reset()
        episode_steps = 0
        collisions = 0
        success = False
        
        for step in range(200):  # Run for 150 steps
            if model_type == "pure_mpc":
                action = model.predict(observation, return_numpy=True)
            elif model_type == "pure_mpc_no_collision":
                action = model.predict(observation, return_numpy=True)
            elif model_type == "mpcrl":
                action = model.predict(observation, False)
                action = [action.acceleration/5, action.steer/(np.pi/3)]
            
            observation, reward, done, truncated, info = env.step(action)
            episode_steps += 1
            
            if VISUALIZATION_MODE:
                env.render()
                print(f"Step {step}, Reward: {reward}")
                if info.get("crashed", False):
                    print("COLLISION DETECTED!")
                # Arrival is checked in the main loop condition above
            
            # Check for collision
            if info.get("crashed", False):
                collisions += 1
            
            # Check for success (reaching the end) using the environment's has_arrived method
            if env.unwrapped.has_arrived(env.unwrapped.controlled_vehicles[0]):
                success = True
                if VISUALIZATION_MODE:
                    print("DESTINATION REACHED! Vehicle has arrived at exit.")
                break
                
            if done or truncated:
                break
        
        return {
            "success": success,
            "collisions": collisions > 0,
            "steps": episode_steps
        }

    def run_evaluation(self):
        results = defaultdict(lambda: {"successes": 0, "collisions": 0, "total_steps": 0})
        
        # Initialize environment
        env = gym.make("intersection-v1", render_mode=RENDER_MODE, config=self.env_config)
        
        for model_type in MODELS_TO_EVALUATE:
            print(f"\nEvaluating {model_type}...")
            
            # Initialize the appropriate model
            if model_type == "pure_mpc":
                model = PureMPC_Agent(env, self.pure_mpc_cfg)
            elif model_type == "pure_mpc_no_collision":
                model = PureMPC_NoCollision_Agent(env, self.pure_mpc_no_collision_cfg)
            elif model_type == "mpcrl":
                model = RefSpeedTrainer(env, self.mpcrl_cfg, self.pure_mpc_cfg)
                # Find the latest saved model
                save_dir = os.path.join(project_root, "saved_models")
                model_files = sorted(glob.glob(f"{save_dir}/*"), key=os.path.getmtime, reverse=True)
                if not model_files:
                    raise FileNotFoundError(f"No saved models found in {save_dir}")
                
                latest_model_path = model_files[0]
                print(f"Loading latest model: {latest_model_path}")
                
                model.load(
                    path=latest_model_path,
                    mpcrl_cfg=self.mpcrl_cfg,
                    version="v0",
                    pure_mpc_cfg=self.pure_mpc_cfg,
                    env=env
                )
            
            # Run evaluation episodes
            for episode in range(N_EPISODES):
                print(f"Episode {episode + 1}/{N_EPISODES}")
                episode_results = self.evaluate_single_episode(model_type, model, env)
                self.update_results(model_type, episode_results, results)
                
                if VISUALIZATION_MODE:
                    print("\nEpisode Results:")
                    print(f"Success: {'Yes' if episode_results['success'] else 'No'}")
                    print(f"Collisions: {'Yes' if episode_results['collisions'] else 'No'}")
                    print(f"Steps taken: {episode_results['steps']}")
                    print("-" * 50)
        
        env.close()
        return results

    def update_results(self, model_type, episode_results, results):
        results[model_type]["successes"] += int(episode_results["success"])
        results[model_type]["collisions"] += int(episode_results["collisions"])
        results[model_type]["total_steps"] += episode_results["steps"]

    def plot_results(self, results, n_episodes):
        if n_episodes == 1:
            return  # Don't create plots for single episodes
            
        models = list(results.keys())
        metrics = {
            "Success Rate": [results[m]["successes"]/n_episodes * 100 for m in models],
            "Collision Rate": [results[m]["collisions"]/n_episodes * 100 for m in models],
            "Average Steps": [results[m]["total_steps"]/n_episodes for m in models]
        }
        
        fig, axes = plt.subplots(1, 3, figsize=(15, 5))
        colors = ['#2ecc71', '#e74c3c', '#3498db']  # Green, Red, Blue
        
        for i, (metric, values) in enumerate(metrics.items()):
            bars = axes[i].bar(models, values, color=colors)
            axes[i].set_title(metric)
            axes[i].set_xticklabels(models, rotation=45)
            if metric.endswith("Rate"):
                axes[i].set_ylabel("Percentage (%)")
            else:
                axes[i].set_ylabel("Steps")
            
            # Add value labels on top of each bar
            for bar in bars:
                height = bar.get_height()
                axes[i].text(bar.get_x() + bar.get_width()/2., height,
                         f'{height:.1f}',
                         ha='center', va='bottom')
        
        plt.tight_layout()
        plot_path = os.path.join(project_root, "model_comparison_results.png")
        plt.savefig(plot_path)
        print(f"\nResults plot saved to: {plot_path}")
        plt.close()

@hydra.main(config_name="cfg", config_path="../config", version_base="1.3")
def main(cfg):
    evaluator = ModelEvaluator(cfg)
    results = evaluator.run_evaluation()
    
    # Print results
    if not VISUALIZATION_MODE:
        print("\nEvaluation Results:")
        print("=" * 50)
        for model_type, metrics in results.items():
            print(f"\nModel: {model_type}")
            print(f"Success Rate: {metrics['successes']/N_EPISODES * 100:.1f}%")
            print(f"Collision Rate: {metrics['collisions']/N_EPISODES * 100:.1f}%")
            print(f"Average Steps: {metrics['total_steps']/N_EPISODES:.1f}")
        
        # Plot results
        evaluator.plot_results(results, N_EPISODES)

if __name__ == "__main__":
    main()